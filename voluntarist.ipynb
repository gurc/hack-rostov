{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f2f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d224faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    df = pd.read_csv(\"a.csv/a.csv\", decimal='.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2105ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pass(df):\n",
    "    return df\n",
    "\n",
    "def filter_tral(df):\n",
    "    result_df = df[df['sure_tral'] == 1]\n",
    "    return result_df\n",
    "\n",
    "def filter_not_tral(df):\n",
    "    result_df = df[df['sure_tral'] == 0]\n",
    "    return result_df\n",
    "\n",
    "def filter_equal(df, frac=1.0):\n",
    "    tral_df = df[df['sure_tral'] == 1]\n",
    "    # print('tral shape', tral_df.shape)\n",
    "    not_tral_df = df[df['sure_tral'] != 1]\n",
    "    not_tral_df = not_tral_df.sample(n=int(tral_df.shape[0]*frac))\n",
    "    # print('not tral shape', not_tral_df.shape)\n",
    "    frames = [tral_df, not_tral_df]\n",
    "    result = pd.concat(frames)\n",
    "    # print('result shape', result.shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc5081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_analyze():\n",
    "    df = load()\n",
    "    tral_df = filter_tral(df)\n",
    "    not_tral_df = filter_not_tral(df)\n",
    "\n",
    "    df.info()\n",
    "    print('********************')\n",
    "    print('ships count: ', df['ship'].unique().shape[0])\n",
    "    print('records count: ', df['record'].unique().shape[0])\n",
    "    print('********************')\n",
    "    print('class disctribution:')\n",
    "    print('not trall class - {:.3}'.format(tral_df.shape[0] / df.shape[0]), '({})'.format(tral_df.shape[0]))\n",
    "    print('trall class - {:.3}'.format(not_tral_df.shape[0] / df.shape[0]), '({})'.format(not_tral_df.shape[0]))\n",
    "    print('********************')\n",
    "    print('missed values:')\n",
    "    for column_name in df.columns:\n",
    "        df[df['ship'] == None]\n",
    "        count = df[df[column_name] == 'None'].shape[0]\n",
    "        print(column_name, count)\n",
    "    print('not tral error rows:', \n",
    "          df[((df['course'] == 'None') | (df['velocity'] == 'None')) & (df['sure_tral'] == 0)].shape[0])\n",
    "    print('tral error rows:', \n",
    "          df[((df['course'] == 'None') | (df['velocity'] == 'None')) & (df['sure_tral'] == 1)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49c722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# general_analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50a2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessing(df, frac=0.01):\n",
    "    if not frac is None:\n",
    "        df_sampled = df.sample(frac=frac)\n",
    "    else:\n",
    "        df_sampled = df\n",
    "    df_clear = df_sampled[df_sampled.velocity != 'None']\n",
    "    df_clear = df_clear[df_clear.course != 'None']\n",
    "    df_clear['velocity'] = df_clear['velocity'].astype(float)\n",
    "    df_clear['course'] = df_clear['course'].astype(int)\n",
    "    return df_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5ac7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    df_x = df[['course', 'velocity']]\n",
    "    df_y = df[['sure_tral']]\n",
    "    return df_x, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14766969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(df_x, df_y):\n",
    "    # print('clf x', df_x.shape)\n",
    "    # print(df_x.head)\n",
    "    # print('clf y', df_y.shape)\n",
    "    # print(df_y.head)\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(df_x, df_y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63156ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(clf, df_x, df_y):\n",
    "#     y_predict = clf.predict(df_x)\n",
    "#     y_real = df_y.to_numpy().reshape((1, -1))\n",
    "#     # print('predict sum', np.sum(y_predict))\n",
    "#     # print('real sum', np.sum(y_real))\n",
    "#     diff = np.sum(y_predict != y_real)\n",
    "#     # print('diff', diff)\n",
    "#     value = diff / len(y_predict)\n",
    "#     # print('len', len(y_predict))\n",
    "#     return value\n",
    "\n",
    "def cross_val_learn(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    score = scores.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0c9ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_features(df):\n",
    "    g = sns.pairplot(df, hue='sure_tral', palette=\"YlGnBu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e607f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_result(train_frac, test_frac, test_filter_fn=filter_pass, train_filter_fn=filter_pass):\n",
    "#     source_df = load()\n",
    "#     train_df = proccessing(source_df, frac=train_frac)\n",
    "#     test_df = proccessing(source_df, frac=test_frac)\n",
    "#     # print('train df shape before filter', train_df.shape)\n",
    "#     train_df=train_filter_fn(train_df)\n",
    "#     # print('train df shape after filter', train_df.shape)\n",
    "#     df_train_x, df_train_y = split(train_df)\n",
    "#     test_df=test_filter_fn(test_df)\n",
    "#     df_test_x, df_test_y = split(test_df)\n",
    "#     clf = learn(df_train_x, df_train_y)\n",
    "#     print(evaluate(clf, df_test_x, df_test_y))\n",
    "#     return clf\n",
    "\n",
    "def get_result(train_frac, test_frac, test_filter_fn=filter_pass, train_filter_fn=filter_pass):\n",
    "    source_df = load()\n",
    "    # train_df = proccessing(source_df, frac=train_frac)\n",
    "    # test_df = proccessing(source_df, frac=test_frac)\n",
    "    clear_df = proccessing(source_df, frac=None)\n",
    "    train_df=train_filter_fn(clear_df)\n",
    "    # print('train df shape after filter', train_df.shape)\n",
    "    df_train_x, df_train_y = split(train_df)\n",
    "    # test_df=test_filter_fn(test_df)\n",
    "    # df_test_x, df_test_y = split(test_df)\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    score = cross_val_learn(clf, df_train_x, df_train_y)\n",
    "    print(score)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fafd1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_many_result():\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_pass)\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_tral)\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_equal)\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_equal, train_filter_fn=filter_equal)\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_pass, train_filter_fn=filter_equal)\n",
    "    clf = get_result(train_frac=0.3, test_frac=1, test_filter_fn=filter_equal, train_filter_fn=filter_equal)\n",
    "    print(clf.predict([[330, 7.56]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb18e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# get_result(1, 1, filter_equal, filter_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41bf9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_analyze():\n",
    "    source_df = load()\n",
    "    train_frac = 0.1\n",
    "    train_filter_fn = filter_equal\n",
    "    train_df = proccessing(source_df, frac=train_frac)\n",
    "    train_df=train_filter_fn(train_df)\n",
    "    df_train_x, df_train_y = split(train_df)\n",
    "    clf = learn(df_train_x, df_train_y)\n",
    "    text_representation = tree.export_text(clf)\n",
    "    print(text_representation)\n",
    "    return train_df\n",
    "\n",
    "def dataset_analyze(train_df):\n",
    "    not_trall = train_df[train_df['sure_tral'] == 0]\n",
    "    trall = train_df[train_df['sure_tral'] == 1]\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharex='all', sharey='all')\n",
    "\n",
    "    ax1.scatter(not_trall['course'], not_trall['velocity'], c='#0000FF', label='not_trall', alpha=0.5)\n",
    "    plt.xlabel('course')\n",
    "    plt.ylabel('velocity')\n",
    "    ax1.legend()\n",
    "    ax2.scatter(trall['course'], trall['velocity'], c='#00FF00', label='trall', alpha=0.5)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "    plt.savefig('fig1.png', dpi=300)\n",
    "   #  plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a4fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# train_df = tree_analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25198fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# dataset_analyze(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c29e3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_plot():\n",
    "    source_df = load()\n",
    "    train_frac = 0.1\n",
    "    train_filter_fn = filter_equal\n",
    "    clear_df = proccessing(source_df, frac=train_frac)\n",
    "    clear_df=train_filter_fn(clear_df)\n",
    "\n",
    "    not_trall = clear_df[clear_df['sure_tral'] == 0]\n",
    "    trall = clear_df[clear_df['sure_tral'] == 1]\n",
    "    \n",
    "    not_trall['velocity'] = not_trall['velocity'].astype(str)\n",
    "    trall['velocity'] = trall['velocity'].astype(str)\n",
    "    \n",
    "    trall_unique = trall['velocity'].value_counts(dropna=False)\n",
    "    trall_values = trall_unique.to_numpy()\n",
    "    trall_indexes = trall_unique.index.to_numpy()\n",
    "    trall_count = np.array((trall_values, trall_indexes)).transpose()\n",
    "    trall_count = trall_count[trall_count[:, 1].argsort()]\n",
    "\n",
    "    not_trall_unique = not_trall['velocity'].value_counts(dropna=False)\n",
    "    not_trall_values = not_trall_unique.to_numpy()\n",
    "    not_trall_indexes = not_trall_unique.index.to_numpy()\n",
    "    not_trall_count = np.array((not_trall_values, not_trall_indexes)).transpose()\n",
    "    \n",
    "    velocity_dict = {}\n",
    "    for i in range(not_trall_count.shape[0]):\n",
    "        velocity_dict[not_trall_count[i, 1]] = [not_trall_count[i, 0], 0]\n",
    "\n",
    "    for i in range(trall_count.shape[0]):\n",
    "        key = str(trall_count[i, 1])\n",
    "        if key in velocity_dict.keys():\n",
    "            velocity_dict[key] = [velocity_dict[key][0], int(trall_count[i, 0])]\n",
    "        else:\n",
    "            velocity_dict[key] = [0, int(trall_count[i, 0])]\n",
    "    \n",
    "    velocities = np.zeros((len(velocity_dict)))\n",
    "    trall_velicities = np.zeros((len(velocity_dict)))\n",
    "    not_trall_velicities = np.zeros((len(velocity_dict)))\n",
    "    list_keys = list(velocity_dict.keys())\n",
    "\n",
    "    for i in range(len(list_keys)):\n",
    "        velocities[i] = float(list_keys[i])\n",
    "        not_trall_velicities [i] = velocity_dict[list_keys[i]][0]\n",
    "        trall_velicities [i] = velocity_dict[list_keys[i]][1]\n",
    "        \n",
    "    # A python dictionary\n",
    "    data = {\"trall\": trall_velicities,\n",
    "\n",
    "            \"not_trall\": not_trall_velicities\n",
    "\n",
    "            };\n",
    "\n",
    "    # Dictionary loaded into a DataFrame       \n",
    "    dataFrame = pd.DataFrame(data=data, index = velocities);\n",
    "    dataFrame = dataFrame.sort_index()\n",
    "\n",
    "    # Draw a vertical bar chart\n",
    "    dataFrame.plot.bar(rot=70, title=\"\");\n",
    "\n",
    "    # from matplotlib.pyplot import figure\n",
    "    plt.rcParams[\"figure.figsize\"] = (25,25)\n",
    "\n",
    "    plt.savefig('fig2.png', dpi=300)\n",
    "    plt.show(block=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d8986e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# get_linear_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d403f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для рисования областей аномалий\n",
    "def run_and_plot(clf, X, outliers_fraction, draw_legend=True, title=''):\n",
    "    \n",
    "    clf.fit(X)\n",
    "    \n",
    "    a_prob =  clf.decision_function(X)\n",
    "    threshold = stats.scoreatpercentile(a_prob, 100 * outliers_fraction)\n",
    "    \n",
    "    #print (a_prob)\n",
    "    \n",
    "    # print ('ошибка  = ' + str( (clf.predict(X) != y).mean()))\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(0, 360, 500), np.linspace(0, 20, 500))\n",
    "    \n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    #print (Z)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    #plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 20), cmap=plt.cm.binary) # plt.cm.Blues_r cmap=plt.cm.Blues_r)\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 20), cmap=plt.cm.binary) # plt.cm.Blues_r cmap=plt.cm.Blues_r)\n",
    "    a_ = plt.contour(xx, yy, Z, levels=[threshold], linewidths=1, colors='yellow')\n",
    "    #plt.contourf(xx, yy, Z, levels=[threshold, Z.max()], colors='#CCDDFF') # CCDDFF\n",
    "    b_ = plt.scatter(X['course'], X['velocity'], c='white')\n",
    "    # c_ = plt.scatter(X[y<0, 0], X[y<0, 1], c='red')\n",
    "    plt.axis('tight')\n",
    "    if draw_legend:\n",
    "        plt.legend(\n",
    "            # [a_.collections[0], b_, c_],\n",
    "            [a_.collections[0], b_],\n",
    "            [u'разделяющая поверхность', u'нормальные объекты', u'выбросы'],\n",
    "            prop=matplotlib.font_manager.FontProperties(size=11), loc='upper right')\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "    plt.savefig('fig3.png', dpi=300)\n",
    "\n",
    "def find_anomaly():\n",
    "    # get dataset not_trall and trall\n",
    "    source_df = load()\n",
    "    train_frac = 0.2\n",
    "    train_filter_fn = filter_equal\n",
    "    clear_df = proccessing(source_df, frac=train_frac)\n",
    "    clear_df=train_filter_fn(clear_df)\n",
    "\n",
    "    not_trall = clear_df[clear_df['sure_tral'] == 0]\n",
    "    trall = clear_df[clear_df['sure_tral'] == 1]\n",
    "\n",
    "    not_trall = not_trall[not_trall.velocity.between(0, 20)]\n",
    "    not_trall = not_trall[['course', 'velocity']]\n",
    "    not_trall.head\n",
    "    \n",
    "    n = 150\n",
    "    outliers_fraction = 0.1\n",
    "    clf  = IsolationForest(n_estimators=n, max_samples=0.5, contamination='auto', max_features=2,\n",
    "                           bootstrap=False, n_jobs=1, random_state=None, verbose=0, warm_start=True)\n",
    "\n",
    "    run_and_plot(clf, not_trall, outliers_fraction=outliers_fraction, draw_legend=True, title='IForest, contamination=0.1, max_features=1.0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57797214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# find_anomaly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "620c657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(source_df, ship=None, record=None):\n",
    "    if not ship is None:\n",
    "        selected_df = source_df[source_df['ship'] == ship]\n",
    "    if not record is None:\n",
    "        selected_df = selected_df[selected_df['record'] == record]\n",
    "\n",
    "    # train_frac = 0.1\n",
    "    # train_filter_fn = filter_equal\n",
    "    # clear_df = proccessing(source_df, frac=train_frac)\n",
    "    # clear_df=train_filter_fn(clear_df)\n",
    "\n",
    "    not_trall = selected_df[selected_df['sure_tral'] == 0]\n",
    "    trall = selected_df[selected_df['sure_tral'] == 1]\n",
    "\n",
    "    # print(not_trall.head)\n",
    "    # print(trall.head)\n",
    "    # 8182    -8263    \n",
    "    # 8264    -8271    \n",
    "    f, ax1 = plt.subplots(1, 1)\n",
    "    ax1.scatter(not_trall['latitude'], not_trall['longitude'], c='#0000FF', label='not_tral', alpha=0.5)\n",
    "    ax1.scatter(trall['latitude'], trall['longitude'], c='#00FF00', label='tral', alpha=0.5)\n",
    "    ax1.legend()\n",
    "    plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "    plt.savefig('trajectory.png', dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bce359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Раскомменитровать для использования\n",
    "# df = load()\n",
    "# plot_trajectory(df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cc6f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_fn():\n",
    "    # train_frac = 1.0\n",
    "    # clear_df = proccessing(source_df, frac=1.0)\n",
    "    head = source_df.head(1000_000)\n",
    "    head = proccessing(head, frac=None)\n",
    "\n",
    "    grouped = head.groupby(['record']).agg(latitude=pd.NamedAgg(column='latitude', aggfunc=lambda x: max(x)-min(x)),\n",
    "                                          sure_tral=pd.NamedAgg(column='latitude', aggfunc=lambda x: max(x)))\n",
    "    print(grouped.head)\n",
    "\n",
    "    not_trall = grouped[grouped['sure_tral'] == 0]\n",
    "    trall = grouped[grouped['sure_tral'] == 1]\n",
    "\n",
    "    # print(not_trall.head)\n",
    "    # print(trall.head)\n",
    "    # 8182    -8263    \n",
    "    # 8264    -8271    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.scatter(not_trall['velocity'], not_trall['sure_tral'], c='#0000FF', label='not_trall', alpha=0.5)\n",
    "    ax1.scatter(trall['velocity'], trall['sure_tral'], c='#00FF00', label='trall', alpha=0.5)\n",
    "    plt.xlabel('velocity')\n",
    "    plt.ylabel('sure_tral')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.scatter(not_trall['latitude'], not_trall['sure_tral'], c='#0000FF', label='not_trall', alpha=0.5)\n",
    "    ax2.scatter(trall['latitude'], trall['sure_tral'], c='#00FF00', label='trall', alpha=0.5)\n",
    "    plt.xlabel('latitude')\n",
    "    plt.ylabel('sure_tral')\n",
    "    ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a253322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_classification():\n",
    "    source_df = load()\n",
    "    df_clear = proccessing(df=source_df, frac=None)\n",
    "\n",
    "    df1 = df_clear\n",
    "    df1 = df1.rename(columns={'ship': 'ship1', 'record': 'record1', 'time': 'time1', \n",
    "                              'latitude': 'latitude1', 'longitude': 'longitude1', \n",
    "                              'course': 'course1', 'velocity': 'velocity1', 'sure_tral': 'sure_tral1'})\n",
    "    df2 = df1.shift(-1)\n",
    "    df2 = df2.rename(columns={'ship1': 'ship2', 'record1': 'record2', 'time1': 'time2', \n",
    "                              'latitude1': 'latitude2', 'longitude1': 'longitude2', \n",
    "                              'course1': 'course2', 'velocity1': 'velocity2', 'sure_tral1': 'sure_tral2'})\n",
    "    df3 = df2.shift(-1)\n",
    "    df3 = df3.rename(columns={'ship2': 'ship3', 'record2': 'record3', 'time2': 'time3', \n",
    "                              'latitude2': 'latitude3', 'longitude2': 'longitude3', \n",
    "                              'course2': 'course3', 'velocity2': 'velocity3', 'sure_tral2': 'sure_tral3'})\n",
    "    df4 = df3.shift(-1)\n",
    "    df4 = df4.rename(columns={'ship3': 'ship4', 'record3': 'record4', 'time3': 'time4', \n",
    "                              'latitude3': 'latitude4', 'longitude3': 'longitude4', \n",
    "                              'course3': 'course4', 'velocity3': 'velocity4', 'sure_tral3': 'sure_tral4'})\n",
    "    df5 = df4.shift(-1)\n",
    "    df5 = df5.rename(columns={'ship4': 'ship5', 'record4': 'record5', 'time4': 'time5', \n",
    "                              'latitude4': 'latitude5', 'longitude4': 'longitude5', \n",
    "                              'course4': 'course5', 'velocity4': 'velocity5', 'sure_tral4': 'sure_tral5'})\n",
    "    df = pd.concat((df1, df2, df3, df4, df5), axis=1)\n",
    "    df = df[df['record1'] == df['record2']]\n",
    "    df = df[df['record2'] == df['record3']]\n",
    "    df = df[df['record3'] == df['record4']]\n",
    "    df = df[df['record4'] == df['record5']]\n",
    "    \n",
    "    # print(df.shape)\n",
    "    \n",
    "    work_df = df[['ship1', 'record1', 'time1', 'time2', 'time3', 'time4', 'time5', \n",
    "                  'latitude1', 'latitude2', 'latitude3', 'latitude4', 'latitude5',\n",
    "                 'longitude1', 'longitude2', 'longitude3', 'longitude4', 'longitude5',\n",
    "                 'course1', 'course2', 'course3', 'course4', 'course5',\n",
    "                 'velocity1', 'velocity2', 'velocity3', 'velocity4', 'velocity5',\n",
    "                 'sure_tral1']]\n",
    "    work_df = work_df.rename(columns={'sure_tral1': 'sure_tral'})\n",
    "    \n",
    "    work_df['latitude5'] = work_df['latitude5'] - work_df['latitude1']\n",
    "    work_df['latitude4'] = work_df['latitude4'] - work_df['latitude1']\n",
    "    work_df['latitude3'] = work_df['latitude3'] - work_df['latitude1']\n",
    "    work_df['latitude2'] = work_df['latitude2'] - work_df['latitude1']\n",
    "    work_df['latitude1'] = work_df['latitude1'] - work_df['latitude1']\n",
    "\n",
    "    work_df['longitude5'] = work_df['longitude5'] - work_df['longitude1']\n",
    "    work_df['longitude4'] = work_df['longitude4'] - work_df['longitude1']\n",
    "    work_df['longitude3'] = work_df['longitude3'] - work_df['longitude1']\n",
    "    work_df['longitude2'] = work_df['longitude2'] - work_df['longitude1']\n",
    "    work_df['longitude1'] = work_df['longitude1'] - work_df['longitude1']\n",
    "\n",
    "    work_df['course5'] = work_df['course5'] - work_df['course1']\n",
    "    work_df['course4'] = work_df['course4'] - work_df['course1']\n",
    "    work_df['course3'] = work_df['course3'] - work_df['course1']\n",
    "    work_df['course2'] = work_df['course2'] - work_df['course1']\n",
    "    work_df['course1'] = work_df['course1'] - work_df['course1']\n",
    "\n",
    "    # print(work_df.head)\n",
    "    # df_sampled = work_df.sample(frac=0.1)\n",
    "    # print(df_sampled.head)\n",
    "    \n",
    "    # Обучение и тестирование на сбалансированной выборке\n",
    "    train_df=filter_equal(work_df, frac=1)\n",
    "    df_train_x = train_df[[ \n",
    "                  'latitude1', 'latitude2', 'latitude3', 'latitude4', 'latitude5',\n",
    "                 'longitude1', 'longitude2', 'longitude3', 'longitude4', 'longitude5',\n",
    "                 'course1', 'course2', 'course3', 'course4', 'course5',\n",
    "                 'velocity1', 'velocity2', 'velocity3', 'velocity4', 'velocity5'\n",
    "    ]]\n",
    "    df_train_y = train_df['sure_tral']\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    learning_result = cross_validate(clf, df_train_x, df_train_y, return_estimator=True)\n",
    "    bal_scores = learning_result['test_score']\n",
    "    bal_score = bal_scores.mean()\n",
    "    trained_clf = learning_result['estimator'][0]\n",
    "    print('Balanced score', bal_score)\n",
    "    \n",
    "    return trained_clf, work_df\n",
    "\n",
    "def inaccuracy_decode(clf, df):\n",
    "    # Расшифровка типов ошибок\n",
    "    test_df=filter_equal(df)\n",
    "    df_test_x = test_df[[\n",
    "                  'latitude1', 'latitude2', 'latitude3', 'latitude4', 'latitude5',\n",
    "                 'longitude1', 'longitude2', 'longitude3', 'longitude4', 'longitude5',\n",
    "                 'course1', 'course2', 'course3', 'course4', 'course5',\n",
    "                 # 'velocity1', 'velocity2', 'velocity3', 'velocity4', 'velocity5'\n",
    "    ]]\n",
    "    df_test_y = test_df[['sure_tral']]\n",
    "\n",
    "    y_predict = clf.predict(df_test_x)\n",
    "    y_real = df_test_y.to_numpy().reshape((1, -1))\n",
    "\n",
    "    diff1 = np.sum(y_predict < y_real) # Когда мы не заметили, что он рыбачил\n",
    "    diff2 = np.sum(y_predict > y_real) # Когда мы ложно заподозрили\n",
    "    diff3 = np.sum(y_predict == y_real) # Когда предсказание совпало с датасетом\n",
    "    # print('diff', diff)\n",
    "    print('Расшифровка предсказаний для отдельных строк:')\n",
    "    value1 = diff1 / len(y_predict)\n",
    "    print('По отчетности вылов был, а система выдала его отсутствие', value1)\n",
    "    value2 = diff2 / len(y_predict)\n",
    "    print('По отчётности вылова не было, а система заподозрила его наличие', value2)\n",
    "    value3 = diff3 / len(y_predict)\n",
    "    print('Данные по отчётам и ответ системы совпали', value3)\n",
    "    test_df['y_pred'] = clf.predict(test_df[[ \n",
    "              'latitude1', 'latitude2', 'latitude3', 'latitude4', 'latitude5',\n",
    "             'longitude1', 'longitude2', 'longitude3', 'longitude4', 'longitude5',\n",
    "             'course1', 'course2', 'course3', 'course4', 'course5',\n",
    "             # 'velocity1', 'velocity2', 'velocity3', 'velocity4', 'velocity5'\n",
    "    ]])\n",
    "    grouped_df_result = test_df.groupby(['record1']).agg(y_real=pd.NamedAgg(column='sure_tral', aggfunc=lambda x: max(x)),\n",
    "                                          y_pred=pd.NamedAgg(column='y_pred', aggfunc=lambda x: max(x)))\n",
    "    r_diff1 = np.sum(grouped_df_result['y_pred'] < grouped_df_result['y_real']) # Когда мы не заметили, что он рыбачил\n",
    "    r_diff2 = np.sum(grouped_df_result['y_pred'] > grouped_df_result['y_real']) # Когда мы ложно заподозрили\n",
    "    r_diff3 = np.sum(grouped_df_result['y_pred'] == grouped_df_result['y_real']) # Когда предсказание совпало с датасетом\n",
    "    r_value1 = r_diff1 / len(grouped_df_result)\n",
    "    print('Расшифровка предсказаний для дней целиком:')\n",
    "    print('По отчетности вылов был, а система выдала его отсутствие', r_value1)\n",
    "    r_value2 = r_diff2 / len(grouped_df_result)\n",
    "    print('По отчётности вылова не было, а система заподозрила его наличие', r_value2)\n",
    "    r_value3 = r_diff3 / len(grouped_df_result)\n",
    "    print('Данные по отчётам и ответ системы совпали', r_value3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac729d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gurcmikhail\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced score 0.950181015119966\n"
     ]
    }
   ],
   "source": [
    "trained_clf, work_df = time_series_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0070006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расшифровка предсказаний для отдельных строк:\n",
      "По отчетности вылов был, а система выдала его отсутствие 0.022724081030790298\n",
      "По отчётности вылова не было, а система заподозрила его наличие 0.08656918161710495\n",
      "Данные по отчётам и ответ системы совпали 0.8907067373521048\n",
      "Расшифровка предсказаний для дней целиком:\n",
      "По отчетности вылов был, а система выдала его отсутствие 0.0\n",
      "По отчётности вылова не было, а система заподозрила его наличие 0.33589936162223055\n",
      "Данные по отчётам и ответ системы совпали 0.6641006383777694\n"
     ]
    }
   ],
   "source": [
    "inaccuracy_decode(trained_clf, work_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
