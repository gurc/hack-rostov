{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    df = pd.read_csv(\"a.csv/a.csv\", decimal='.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pass(df):\n",
    "    return df\n",
    "\n",
    "def filter_tral(df):\n",
    "    result_df = df[df['sure_tral'] == 1]\n",
    "    return result_df\n",
    "\n",
    "def filter_not_tral(df):\n",
    "    result_df = df[df['sure_tral'] == 0]\n",
    "    return result_df\n",
    "\n",
    "def filter_equal(df, frac=1.0):\n",
    "    tral_df = df[df['sure_tral'] == 1]\n",
    "    # print('tral shape', tral_df.shape)\n",
    "    not_tral_df = df[df['sure_tral'] != 1]\n",
    "    not_tral_df = not_tral_df.sample(n=int(tral_df.shape[0]*frac))\n",
    "    # print('not tral shape', not_tral_df.shape)\n",
    "    frames = [tral_df, not_tral_df]\n",
    "    result = pd.concat(frames)\n",
    "    # print('result shape', result.shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_analyze(df):\n",
    "    tral_df = filter_tral(df)\n",
    "    not_tral_df = filter_not_tral(df)\n",
    "\n",
    "    df.info()\n",
    "    print('********************')\n",
    "    print('ships count: ', df['ship'].unique().shape[0])\n",
    "    print('records count: ', df['record'].unique().shape[0])\n",
    "    print('********************')\n",
    "    print('class disctribution:')\n",
    "    print('not trall class - {:.3}'.format(tral_df.shape[0] / df.shape[0]), '({})'.format(tral_df.shape[0]))\n",
    "    print('trall class - {:.3}'.format(not_tral_df.shape[0] / df.shape[0]), '({})'.format(not_tral_df.shape[0]))\n",
    "    print('********************')\n",
    "    print('missed values:')\n",
    "    for column_name in df.columns:\n",
    "        df[df['ship'] == None]\n",
    "        count = df[df[column_name] == 'None'].shape[0]\n",
    "        print(column_name, count)\n",
    "    print('not tral error rows:', \n",
    "          df[((df['course'] == 'None') | (df['velocity'] == 'None')) & (df['sure_tral'] == 0)].shape[0])\n",
    "    print('tral error rows:', \n",
    "          df[((df['course'] == 'None') | (df['velocity'] == 'None')) & (df['sure_tral'] == 1)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# df = load()\n",
    "# general_analyze(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccessing(df, frac=0.01):\n",
    "    if not frac is None:\n",
    "        df_sampled = df.sample(frac=frac)\n",
    "    else:\n",
    "        df_sampled = df\n",
    "    df_clear = df_sampled[df_sampled.velocity != 'None']\n",
    "    df_clear = df_clear[df_clear.course != 'None']\n",
    "    df_clear['velocity'] = df_clear['velocity'].astype(float)\n",
    "    df_clear['course'] = df_clear['course'].astype(int)\n",
    "    return df_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    df_x = df[['course', 'velocity']]\n",
    "    df_y = df[['sure_tral']]\n",
    "    return df_x, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(df_x, df_y):\n",
    "    # print('clf x', df_x.shape)\n",
    "    # print(df_x.head)\n",
    "    # print('clf y', df_y.shape)\n",
    "    # print(df_y.head)\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(df_x, df_y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(clf, df_x, df_y):\n",
    "#     y_predict = clf.predict(df_x)\n",
    "#     y_real = df_y.to_numpy().reshape((1, -1))\n",
    "#     # print('predict sum', np.sum(y_predict))\n",
    "#     # print('real sum', np.sum(y_real))\n",
    "#     diff = np.sum(y_predict != y_real)\n",
    "#     # print('diff', diff)\n",
    "#     value = diff / len(y_predict)\n",
    "#     # print('len', len(y_predict))\n",
    "#     return value\n",
    "\n",
    "def cross_val_learn(clf, X, y):\n",
    "    learning_result = cross_validate(clf, X, y, return_estimator=True)\n",
    "    scores = learning_result['test_score']\n",
    "    score = scores.mean()\n",
    "    trained_clf = learning_result['estimator'][0]\n",
    "    \n",
    "    return score, trained_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_features(df):\n",
    "    g = sns.pairplot(df, hue='sure_tral', palette=\"YlGnBu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_result(train_frac, test_frac, test_filter_fn=filter_pass, train_filter_fn=filter_pass):\n",
    "#     source_df = load()\n",
    "#     train_df = proccessing(source_df, frac=train_frac)\n",
    "#     test_df = proccessing(source_df, frac=test_frac)\n",
    "#     # print('train df shape before filter', train_df.shape)\n",
    "#     train_df=train_filter_fn(train_df)\n",
    "#     # print('train df shape after filter', train_df.shape)\n",
    "#     df_train_x, df_train_y = split(train_df)\n",
    "#     test_df=test_filter_fn(test_df)\n",
    "#     df_test_x, df_test_y = split(test_df)\n",
    "#     clf = learn(df_train_x, df_train_y)\n",
    "#     print(evaluate(clf, df_test_x, df_test_y))\n",
    "#     return clf\n",
    "\n",
    "def get_result(train_frac, test_frac, test_filter_fn=filter_pass, train_filter_fn=filter_pass):\n",
    "    source_df = load()\n",
    "    # train_df = proccessing(source_df, frac=train_frac)\n",
    "    # test_df = proccessing(source_df, frac=test_frac)\n",
    "    clear_df = proccessing(source_df, frac=None)\n",
    "    train_df=train_filter_fn(clear_df)\n",
    "    # print('train df shape after filter', train_df.shape)\n",
    "    df_train_x, df_train_y = split(train_df)\n",
    "    # test_df=test_filter_fn(test_df)\n",
    "    # df_test_x, df_test_y = split(test_df)\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    score, trained_clf = cross_val_learn(clf, df_train_x, df_train_y)\n",
    "    print(score)\n",
    "    return trained_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_many_result():\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_pass)\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_tral)\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_equal)\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_equal, train_filter_fn=filter_equal)\n",
    "    get_result(train_frac=0.1, test_frac=1, test_filter_fn=filter_pass, train_filter_fn=filter_equal)\n",
    "    clf = get_result(train_frac=0.3, test_frac=1, test_filter_fn=filter_equal, train_filter_fn=filter_equal)\n",
    "    print(clf.predict([[330, 7.56]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(clf):\n",
    "    source_df = pd.read_csv(\"2people_with_txt_samples/control.csv\", decimal='.')\n",
    "    source_df.head\n",
    "    # source_df['sure_tral']='x'\n",
    "    # general_analyze(source_df)\n",
    "\n",
    "    df_clear = source_df\n",
    "    df_clear['velocity'] = df_clear['velocity'].replace('None', '0.0')\n",
    "    df_clear['course'] = df_clear['course'].replace('None', '0')\n",
    "    df_clear['velocity'] = df_clear['velocity'].astype(float)\n",
    "    df_clear['course'] = df_clear['course'].astype(int)\n",
    "    df_result = df_clear\n",
    "    df_result['sure_tral'] = clf.predict(df_clear[['course', 'velocity']])\n",
    "    df_result.head\n",
    "\n",
    "    result = df_result[[\"record\", \"sure_tral\"]]\n",
    "    result = result.groupby(by=\"record\").max()\n",
    "    result.to_csv(\"out.txt\", sep=' ', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-6f8dd75de568>:16: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  source_df = load()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9690267545560296\n"
     ]
    }
   ],
   "source": [
    "# раскомменитровать для использования\n",
    "clf = get_result(1, 1, filter_equal, filter_equal)\n",
    "# validate(clf):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_analyze():\n",
    "    source_df = load()\n",
    "    train_frac = 0.1\n",
    "    train_filter_fn = filter_equal\n",
    "    train_df = proccessing(source_df, frac=train_frac)\n",
    "    train_df=train_filter_fn(train_df)\n",
    "    df_train_x, df_train_y = split(train_df)\n",
    "    clf = learn(df_train_x, df_train_y)\n",
    "    text_representation = tree.export_text(clf)\n",
    "    print(text_representation)\n",
    "    return train_df\n",
    "\n",
    "def dataset_analyze(train_df):\n",
    "    not_trall = train_df[train_df['sure_tral'] == 0]\n",
    "    trall = train_df[train_df['sure_tral'] == 1]\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, sharex='all', sharey='all')\n",
    "\n",
    "    ax1.scatter(not_trall['course'], not_trall['velocity'], c='#0000FF', label='not_trall', alpha=0.5)\n",
    "    plt.xlabel('course')\n",
    "    plt.ylabel('velocity')\n",
    "    ax1.legend()\n",
    "    ax2.scatter(trall['course'], trall['velocity'], c='#00FF00', label='trall', alpha=0.5)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "    plt.savefig('fig1.png', dpi=300)\n",
    "   #  plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# train_df = tree_analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# dataset_analyze(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_plot():\n",
    "    source_df = load()\n",
    "    train_frac = 0.1\n",
    "    train_filter_fn = filter_equal\n",
    "    clear_df = proccessing(source_df, frac=train_frac)\n",
    "    clear_df=train_filter_fn(clear_df)\n",
    "\n",
    "    not_trall = clear_df[clear_df['sure_tral'] == 0]\n",
    "    trall = clear_df[clear_df['sure_tral'] == 1]\n",
    "    \n",
    "    not_trall['velocity'] = not_trall['velocity'].astype(str)\n",
    "    trall['velocity'] = trall['velocity'].astype(str)\n",
    "    \n",
    "    trall_unique = trall['velocity'].value_counts(dropna=False)\n",
    "    trall_values = trall_unique.to_numpy()\n",
    "    trall_indexes = trall_unique.index.to_numpy()\n",
    "    trall_count = np.array((trall_values, trall_indexes)).transpose()\n",
    "    trall_count = trall_count[trall_count[:, 1].argsort()]\n",
    "\n",
    "    not_trall_unique = not_trall['velocity'].value_counts(dropna=False)\n",
    "    not_trall_values = not_trall_unique.to_numpy()\n",
    "    not_trall_indexes = not_trall_unique.index.to_numpy()\n",
    "    not_trall_count = np.array((not_trall_values, not_trall_indexes)).transpose()\n",
    "    \n",
    "    velocity_dict = {}\n",
    "    for i in range(not_trall_count.shape[0]):\n",
    "        velocity_dict[not_trall_count[i, 1]] = [not_trall_count[i, 0], 0]\n",
    "\n",
    "    for i in range(trall_count.shape[0]):\n",
    "        key = str(trall_count[i, 1])\n",
    "        if key in velocity_dict.keys():\n",
    "            velocity_dict[key] = [velocity_dict[key][0], int(trall_count[i, 0])]\n",
    "        else:\n",
    "            velocity_dict[key] = [0, int(trall_count[i, 0])]\n",
    "    \n",
    "    velocities = np.zeros((len(velocity_dict)))\n",
    "    trall_velicities = np.zeros((len(velocity_dict)))\n",
    "    not_trall_velicities = np.zeros((len(velocity_dict)))\n",
    "    list_keys = list(velocity_dict.keys())\n",
    "\n",
    "    for i in range(len(list_keys)):\n",
    "        velocities[i] = float(list_keys[i])\n",
    "        not_trall_velicities [i] = velocity_dict[list_keys[i]][0]\n",
    "        trall_velicities [i] = velocity_dict[list_keys[i]][1]\n",
    "        \n",
    "    # A python dictionary\n",
    "    data = {\"trall\": trall_velicities,\n",
    "\n",
    "            \"not_trall\": not_trall_velicities\n",
    "\n",
    "            };\n",
    "\n",
    "    # Dictionary loaded into a DataFrame       \n",
    "    dataFrame = pd.DataFrame(data=data, index = velocities);\n",
    "    dataFrame = dataFrame.sort_index()\n",
    "\n",
    "    # Draw a vertical bar chart\n",
    "    dataFrame.plot.bar(rot=70, title=\"\");\n",
    "\n",
    "    # from matplotlib.pyplot import figure\n",
    "    plt.rcParams[\"figure.figsize\"] = (25,25)\n",
    "\n",
    "    plt.savefig('fig2.png', dpi=300)\n",
    "    plt.show(block=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# get_linear_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для рисования областей аномалий\n",
    "def run_and_plot(clf, X, outliers_fraction, draw_legend=True, title=''):\n",
    "    \n",
    "    clf.fit(X)\n",
    "    \n",
    "    a_prob =  clf.decision_function(X)\n",
    "    threshold = stats.scoreatpercentile(a_prob, 100 * outliers_fraction)\n",
    "    \n",
    "    #print (a_prob)\n",
    "    \n",
    "    # print ('ошибка  = ' + str( (clf.predict(X) != y).mean()))\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(0, 360, 500), np.linspace(0, 20, 500))\n",
    "    \n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    #print (Z)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    #plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 20), cmap=plt.cm.binary) # plt.cm.Blues_r cmap=plt.cm.Blues_r)\n",
    "    plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 20), cmap=plt.cm.binary) # plt.cm.Blues_r cmap=plt.cm.Blues_r)\n",
    "    a_ = plt.contour(xx, yy, Z, levels=[threshold], linewidths=1, colors='yellow')\n",
    "    #plt.contourf(xx, yy, Z, levels=[threshold, Z.max()], colors='#CCDDFF') # CCDDFF\n",
    "    b_ = plt.scatter(X['course'], X['velocity'], c='white')\n",
    "    # c_ = plt.scatter(X[y<0, 0], X[y<0, 1], c='red')\n",
    "    plt.axis('tight')\n",
    "    if draw_legend:\n",
    "        plt.legend(\n",
    "            # [a_.collections[0], b_, c_],\n",
    "            [a_.collections[0], b_],\n",
    "            [u'разделяющая поверхность', u'нормальные объекты', u'выбросы'],\n",
    "            prop=matplotlib.font_manager.FontProperties(size=11), loc='upper right')\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (12,12)\n",
    "    plt.savefig('fig3.png', dpi=300)\n",
    "\n",
    "def find_anomaly():\n",
    "    # get dataset not_trall and trall\n",
    "    source_df = load()\n",
    "    train_frac = 0.2\n",
    "    train_filter_fn = filter_equal\n",
    "    clear_df = proccessing(source_df, frac=train_frac)\n",
    "    clear_df=train_filter_fn(clear_df)\n",
    "\n",
    "    not_trall = clear_df[clear_df['sure_tral'] == 0]\n",
    "    trall = clear_df[clear_df['sure_tral'] == 1]\n",
    "\n",
    "    not_trall = not_trall[not_trall.velocity.between(0, 20)]\n",
    "    not_trall = not_trall[['course', 'velocity']]\n",
    "    not_trall.head\n",
    "    \n",
    "    n = 150\n",
    "    outliers_fraction = 0.1\n",
    "    clf  = IsolationForest(n_estimators=n, max_samples=0.5, contamination='auto', max_features=2,\n",
    "                           bootstrap=False, n_jobs=1, random_state=None, verbose=0, warm_start=True)\n",
    "\n",
    "    run_and_plot(clf, not_trall, outliers_fraction=outliers_fraction, draw_legend=True, title='IForest, contamination=0.1, max_features=1.0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# раскомменитровать для использования\n",
    "# find_anomaly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(source_df, ship=None, record=None):\n",
    "    if not ship is None:\n",
    "        selected_df = source_df[source_df['ship'] == ship]\n",
    "    if not record is None:\n",
    "        selected_df = selected_df[selected_df['record'] == record]\n",
    "\n",
    "    # train_frac = 0.1\n",
    "    # train_filter_fn = filter_equal\n",
    "    # clear_df = proccessing(source_df, frac=train_frac)\n",
    "    # clear_df=train_filter_fn(clear_df)\n",
    "\n",
    "    not_trall = selected_df[selected_df['sure_tral'] == 0]\n",
    "    trall = selected_df[selected_df['sure_tral'] == 1]\n",
    "\n",
    "    # print(not_trall.head)\n",
    "    # print(trall.head)\n",
    "    # 8182    -8263    \n",
    "    # 8264    -8271    \n",
    "    f, ax1 = plt.subplots(1, 1)\n",
    "    ax1.scatter(not_trall['latitude'], not_trall['longitude'], c='#0000FF', label='not_tral', alpha=0.5)\n",
    "    ax1.scatter(trall['latitude'], trall['longitude'], c='#00FF00', label='tral', alpha=0.5)\n",
    "    ax1.legend()\n",
    "    plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "    plt.savefig('trajectory.png', dpi=300)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Раскомменитровать для использования\n",
    "# df = load()\n",
    "# plot_trajectory(df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_fn():\n",
    "    # train_frac = 1.0\n",
    "    # clear_df = proccessing(source_df, frac=1.0)\n",
    "    head = source_df.head(1000_000)\n",
    "    head = proccessing(head, frac=None)\n",
    "\n",
    "    grouped = head.groupby(['record']).agg(latitude=pd.NamedAgg(column='latitude', aggfunc=lambda x: max(x)-min(x)),\n",
    "                                          sure_tral=pd.NamedAgg(column='latitude', aggfunc=lambda x: max(x)))\n",
    "    print(grouped.head)\n",
    "\n",
    "    not_trall = grouped[grouped['sure_tral'] == 0]\n",
    "    trall = grouped[grouped['sure_tral'] == 1]\n",
    "\n",
    "    # print(not_trall.head)\n",
    "    # print(trall.head)\n",
    "    # 8182    -8263    \n",
    "    # 8264    -8271    \n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.scatter(not_trall['velocity'], not_trall['sure_tral'], c='#0000FF', label='not_trall', alpha=0.5)\n",
    "    ax1.scatter(trall['velocity'], trall['sure_tral'], c='#00FF00', label='trall', alpha=0.5)\n",
    "    plt.xlabel('velocity')\n",
    "    plt.ylabel('sure_tral')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.scatter(not_trall['latitude'], not_trall['sure_tral'], c='#0000FF', label='not_trall', alpha=0.5)\n",
    "    ax2.scatter(trall['latitude'], trall['sure_tral'], c='#00FF00', label='trall', alpha=0.5)\n",
    "    plt.xlabel('latitude')\n",
    "    plt.ylabel('sure_tral')\n",
    "    ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_classification():\n",
    "    source_df = load()\n",
    "    df_clear = proccessing(df=source_df, frac=None)\n",
    "\n",
    "    df1 = df_clear\n",
    "    df1 = df1.rename(columns={'ship': 'ship1', 'record': 'record1', 'time': 'time1', \n",
    "                              'latitude': 'latitude1', 'longitude': 'longitude1', \n",
    "                              'course': 'course1', 'velocity': 'velocity1', 'sure_tral': 'sure_tral1'})\n",
    "    df2 = df1.shift(-1)\n",
    "    df2 = df2.rename(columns={'ship1': 'ship2', 'record1': 'record2', 'time1': 'time2', \n",
    "                              'latitude1': 'latitude2', 'longitude1': 'longitude2', \n",
    "                              'course1': 'course2', 'velocity1': 'velocity2', 'sure_tral1': 'sure_tral2'})\n",
    "    df3 = df2.shift(-1)\n",
    "    df3 = df3.rename(columns={'ship2': 'ship3', 'record2': 'record3', 'time2': 'time3', \n",
    "                              'latitude2': 'latitude3', 'longitude2': 'longitude3', \n",
    "                              'course2': 'course3', 'velocity2': 'velocity3', 'sure_tral2': 'sure_tral3'})\n",
    "    df4 = df3.shift(-1)\n",
    "    df4 = df4.rename(columns={'ship3': 'ship4', 'record3': 'record4', 'time3': 'time4', \n",
    "                              'latitude3': 'latitude4', 'longitude3': 'longitude4', \n",
    "                              'course3': 'course4', 'velocity3': 'velocity4', 'sure_tral3': 'sure_tral4'})\n",
    "    df5 = df4.shift(-1)\n",
    "    df5 = df5.rename(columns={'ship4': 'ship5', 'record4': 'record5', 'time4': 'time5', \n",
    "                              'latitude4': 'latitude5', 'longitude4': 'longitude5', \n",
    "                              'course4': 'course5', 'velocity4': 'velocity5', 'sure_tral4': 'sure_tral5'})\n",
    "    df = pd.concat((df1, df2, df3, df4, df5), axis=1)\n",
    "    df = df[df['record1'] == df['record2']]\n",
    "    df = df[df['record2'] == df['record3']]\n",
    "    df = df[df['record3'] == df['record4']]\n",
    "    df = df[df['record4'] == df['record5']]\n",
    "    \n",
    "    # print(df.shape)\n",
    "    \n",
    "    work_df = df[['ship1', 'record1', 'time1', 'time2', 'time3', 'time4', 'time5', \n",
    "                  'latitude1', 'latitude2', 'latitude3', 'latitude4', 'latitude5',\n",
    "                 'longitude1', 'longitude2', 'longitude3', 'longitude4', 'longitude5',\n",
    "                 'course1', 'course2', 'course3', 'course4', 'course5',\n",
    "                 # 'velocity1', 'velocity2', 'velocity3', 'velocity4', 'velocity5',\n",
    "                 'sure_tral1']]\n",
    "    work_df = work_df.rename(columns={'sure_tral1': 'sure_tral'})\n",
    "    \n",
    "    work_df['latitude5'] = work_df['latitude5'] - work_df['latitude1']\n",
    "    work_df['latitude4'] = work_df['latitude4'] - work_df['latitude1']\n",
    "    work_df['latitude3'] = work_df['latitude3'] - work_df['latitude1']\n",
    "    work_df['latitude2'] = work_df['latitude2'] - work_df['latitude1']\n",
    "    work_df['latitude1'] = work_df['latitude1'] - work_df['latitude1']\n",
    "\n",
    "    work_df['longitude5'] = work_df['longitude5'] - work_df['longitude1']\n",
    "    work_df['longitude4'] = work_df['longitude4'] - work_df['longitude1']\n",
    "    work_df['longitude3'] = work_df['longitude3'] - work_df['longitude1']\n",
    "    work_df['longitude2'] = work_df['longitude2'] - work_df['longitude1']\n",
    "    work_df['longitude1'] = work_df['longitude1'] - work_df['longitude1']\n",
    "\n",
    "    work_df['course5'] = work_df['course5'] - work_df['course1']\n",
    "    work_df['course4'] = work_df['course4'] - work_df['course1']\n",
    "    work_df['course3'] = work_df['course3'] - work_df['course1']\n",
    "    work_df['course2'] = work_df['course2'] - work_df['course1']\n",
    "    work_df['course1'] = work_df['course1'] - work_df['course1']\n",
    "\n",
    "    # print(work_df.head)\n",
    "    # df_sampled = work_df.sample(frac=0.1)\n",
    "    # print(df_sampled.head)\n",
    "    \n",
    "    # Обучение и тестирование на сбалансированной выборке\n",
    "    train_df=filter_equal(work_df, frac=1)\n",
    "    df_train_x = train_df[[ \n",
    "                  'latitude1', 'latitude2', 'latitude3', 'latitude4', 'latitude5',\n",
    "                 'longitude1', 'longitude2', 'longitude3', 'longitude4', 'longitude5',\n",
    "                 'course1', 'course2', 'course3', 'course4', 'course5',\n",
    "                 # 'velocity1', 'velocity2', 'velocity3', 'velocity4', 'velocity5'\n",
    "    ]]\n",
    "    df_train_y = train_df['sure_tral']\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    learning_result = cross_validate(clf, df_train_x, df_train_y, return_estimator=True)\n",
    "    bal_scores = learning_result['test_score']\n",
    "    bal_score = bal_scores.mean()\n",
    "    trained_clf = learning_result['estimator'][0]\n",
    "    print('Balanced score', bal_score)\n",
    "    \n",
    "    return trained_clf, work_df\n",
    "\n",
    "def inaccuracy_decode(clf, df):\n",
    "    # Расшифровка типов ошибок\n",
    "    test_df=filter_equal(df)\n",
    "    df_test_x = test_df[[\n",
    "                  'latitude1', 'latitude2', 'latitude3', 'latitude4', 'latitude5',\n",
    "                 'longitude1', 'longitude2', 'longitude3', 'longitude4', 'longitude5',\n",
    "                 'course1', 'course2', 'course3', 'course4', 'course5',\n",
    "                 # 'velocity1', 'velocity2', 'velocity3', 'velocity4', 'velocity5'\n",
    "    ]]\n",
    "    df_test_y = test_df[['sure_tral']]\n",
    "\n",
    "    y_predict = clf.predict(df_test_x)\n",
    "    y_real = df_test_y.to_numpy().reshape((1, -1))\n",
    "\n",
    "    diff1 = np.sum(y_predict < y_real) # Когда мы не заметили, что он рыбачил\n",
    "    diff2 = np.sum(y_predict > y_real) # Когда мы ложно заподозрили\n",
    "    diff3 = np.sum(y_predict == y_real) # Когда предсказание совпало с датасетом\n",
    "    # print('diff', diff)\n",
    "    print('Расшифровка предсказаний для отдельных строк:')\n",
    "    value1 = diff1 / len(y_predict)\n",
    "    print('По отчетности вылов был, а система выдала его отсутствие', value1)\n",
    "    value2 = diff2 / len(y_predict)\n",
    "    print('По отчётности вылова не было, а система заподозрила его наличие', value2)\n",
    "    value3 = diff3 / len(y_predict)\n",
    "    print('Данные по отчётам и ответ системы совпали', value3)\n",
    "    test_df['y_pred'] = clf.predict(test_df[[ \n",
    "              'latitude1', 'latitude2', 'latitude3', 'latitude4', 'latitude5',\n",
    "             'longitude1', 'longitude2', 'longitude3', 'longitude4', 'longitude5',\n",
    "             'course1', 'course2', 'course3', 'course4', 'course5',\n",
    "             # 'velocity1', 'velocity2', 'velocity3', 'velocity4', 'velocity5'\n",
    "    ]])\n",
    "    grouped_df_result = test_df.groupby(['record1']).agg(y_real=pd.NamedAgg(column='sure_tral', aggfunc=lambda x: max(x)),\n",
    "                                          y_pred=pd.NamedAgg(column='y_pred', aggfunc=lambda x: max(x)))\n",
    "    r_diff1 = np.sum(grouped_df_result['y_pred'] < grouped_df_result['y_real']) # Когда мы не заметили, что он рыбачил\n",
    "    r_diff2 = np.sum(grouped_df_result['y_pred'] > grouped_df_result['y_real']) # Когда мы ложно заподозрили\n",
    "    r_diff3 = np.sum(grouped_df_result['y_pred'] == grouped_df_result['y_real']) # Когда предсказание совпало с датасетом\n",
    "    r_value1 = r_diff1 / len(grouped_df_result)\n",
    "    print('Расшифровка предсказаний для дней целиком:')\n",
    "    print('По отчетности вылов был, а система выдала его отсутствие', r_value1)\n",
    "    r_value2 = r_diff2 / len(grouped_df_result)\n",
    "    print('По отчётности вылова не было, а система заподозрила его наличие', r_value2)\n",
    "    r_value3 = r_diff3 / len(grouped_df_result)\n",
    "    print('Данные по отчётам и ответ системы совпали', r_value3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-6f8dd75de568>:16: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  source_df = load()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9689336952307096\n"
     ]
    }
   ],
   "source": [
    "# Раскомменитровать для использования\n",
    "# trained_clf, work_df = time_series_classification()\n",
    "clf = get_result(1, 1, filter_equal, filter_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_for_predict(df):\n",
    "    df_sampled = df\n",
    "    df_clear = df_sampled[df_sampled.velocity != 'None']\n",
    "    df_clear = df_clear[df_clear.course != 'None']\n",
    "    df_clear['velocity'] = df_clear['velocity'].astype(float)\n",
    "    df_clear['course'] = df_clear['course'].astype(int)\n",
    "    return df_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, clf):\n",
    "    full_df = df\n",
    "    predict_df = proc_for_predict(df)\n",
    "    predict_x = predict_df[['course', 'velocity']]\n",
    "    predict_result = clf.predict(predict_x)\n",
    "    predict_df[\"predict\"] = predict_result\n",
    "    predict_se = predict_df[\"predict\"]\n",
    "\n",
    "    full_df[\"predict\"] = predict_se\n",
    "    full_df[\"predict\"] = full_df[\"predict\"].fillna(0.49)\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mean(df):\n",
    "    print(\"Находим средние значения за день: \")\n",
    "    df[[\"predict_mean\"]] = -1\n",
    "    df[[\"predict_round_mean\"]] = -1\n",
    "    uniques = df[\"record\"].unique()\n",
    "    for unique in tqdm(uniques):\n",
    "        test = df[df[\"record\"] == unique]\n",
    "        mean = test[\"predict\"].mean()\n",
    "        df.loc[df[\"record\"] == unique, \"predict_mean\"] = mean\n",
    "        mean = round(mean)\n",
    "        df.loc[df[\"record\"] == unique, \"predict_round_mean\"] = mean\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_predict(df, clf):\n",
    "    df = predict(df, clf)\n",
    "    df = predict_mean(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Раскомменитровать для использования\n",
    "# inaccuracy_decode(trained_clf, work_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-f8174577d090>:1: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  train = load()\n",
      "<ipython-input-70-c9c44469eaa4>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_df[\"predict\"] = predict_se\n",
      "<ipython-input-70-c9c44469eaa4>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_df[\"predict\"] = full_df[\"predict\"].fillna(0.49)\n",
      "/home/xiclit/miniconda3/envs/HK/lib/python3.8/site-packages/pandas/core/frame.py:3640: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[col] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Находим средние значения за день: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/606 [00:00<?, ?it/s]/home/xiclit/miniconda3/envs/HK/lib/python3.8/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "100%|██████████| 606/606 [00:02<00:00, 219.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train = load()\n",
    "df = full_predict(train, clf)\n",
    "\n",
    "# valid = pd.read_csv(\"2people_with_txt_samples/control.csv\", decimal='.')\n",
    "# df = full_predict(valid, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ships = df[\"ship\"].unique()\n",
    "selected_ship = widgets.Select(\n",
    "    options=ships,\n",
    "    value=ships[0],\n",
    "    rows=10,\n",
    "    description=\"Корабли:\",\n",
    "    disabled=False\n",
    ")\n",
    "first_ship_records = df.loc[df[\"ship\"] == selected_ship.value, \"record\"].unique()\n",
    "selected_rec = widgets.Select(\n",
    "    options=first_ship_records,\n",
    "    value=first_ship_records[0],\n",
    "    rows=10,\n",
    "    description=\"Записи:\",\n",
    "    disabled=False\n",
    ")\n",
    "fishing = widgets.Label(value=\"\")\n",
    "report = widgets.Button(\n",
    "    description=\"Отчет\",\n",
    "    disabled='sure_tral' not in df.columns,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "output2 = widgets.Output(\n",
    "    layout=widgets.Layout(width='calc(100% + 14ex)')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_selected_ship_change(change):\n",
    "    with output2:\n",
    "        selected_rec.options = df.loc[df[\"ship\"] == selected_ship.value, \"record\"].unique()\n",
    "        output2.clear_output()\n",
    "        \n",
    "def on_selected_record_change(change):\n",
    "    with output2:\n",
    "        output2.clear_output()\n",
    "        rec = df[df[\"ship\"] == selected_ship.value][df[\"record\"] == selected_rec.value]\n",
    "        if rec[\"predict_round_mean\"].mean() == 1:\n",
    "            fishing.value = \"Рыбачит!\"\n",
    "        else:\n",
    "            fishing.value = \"Отдыхает!\"\n",
    "        print(rec)\n",
    "        \n",
    "\n",
    "def report_button(b):\n",
    "    with output2:\n",
    "        output2.clear_output()\n",
    "        recs = df[df[\"sure_tral\"] == 0]\n",
    "        recs = recs[recs[\"predict_round_mean\"] == 1]\n",
    "        recs = recs.groupby(by=\"record\").first()\n",
    "        recs = recs.groupby(by=\"ship\").mean()\n",
    "        recs = recs.nlargest(100, \"predict_mean\")\n",
    "        print(len(recs))\n",
    "        for i in recs.index.array:\n",
    "            print(\"Корабль: \", i, \"   Шансы: \", recs.loc[i][\"predict_mean\"])\n",
    "        \n",
    "\n",
    "report.on_click(report_button)\n",
    "selected_ship.observe(on_selected_ship_change, names='value')\n",
    "selected_rec.observe(on_selected_record_change, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78931489166431aac5ac1a2bf11b930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Select(description='Корабли:', options=(1, 2, 3), rows=10, value=1), Select(description='Записи…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc8e1770e2049a0b8d49509ea55a343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(width='calc(100% + 14ex)'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(widgets.HBox([selected_ship, selected_rec, widgets.VBox([fishing, report])]), output2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
